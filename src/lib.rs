
//! This crate allows developers to utilize our differentially private secure aggregation
//! of gradient vectors to build federated machine learning systems.
//! For an overview of the functionality see [here](https://github.com/dpsa-project/overview).
//! The reference implementation for using this crate are our [python bindings](https://github.com/dpsa-project/dpsa4fl-bindings.py).
//!
//! # Description of the system
//! We assume the existence of a federated machine learning framework, federated by a single server, henceforth "controller".
//! In a privacy-irrelevant setting, this server sees all client submitted gradients in plain text,
//! which makes a variety of attacks possible to reconstruct original user data.
//! dpsa4fl implements an alternative gradient aggregation mechanism which makes it possible
//! to do federated ML in the case where the controller cannot be trusted.
//!
//! This is done by introducing two aggregator servers, both of which participate in the aggregation.
//! By splitting the gradient vector, and sending half of its information to each aggregator, none of
//! them has access to the plain text gradient. Both compute the sum of the gradients submitted by all
//! clients and add noise. The reconstructed noised sum of client gradients is then collected by the
//! controller. As long as at least one aggregator follows the protocol (e.g. is controlled by a trusted
//! party), no participant (particularly neither the controller) has access to unnoised user data.
//!
//! # Components
//! Our aggregation mechanism requires multiple components:
//!  - Two aggregators, responsible for the aggregation and noising of gradients. The code for these
//!    is our [janus fork](https://github.com/dpsa-project/janus), as well as an accompanying [executable](https://github.com/dpsa-project/dpsa4fl-janus-tasks). To setup a local instance of two configured aggregator servers, see [this repo](https://github.com/dpsa-project/dpsa4fl-testing-infrastructure).
//!  - Code for the controller to talk with the aggregators, found in [crate::controller].
//!  - Code for the clients to talk with the aggregators, found in [crate::client].
//!
//! # How to use this crate
//! Assuming that a janus instance is running, this crate provides code for both controller and clients
//! to communicate with it. The communication should follow the following protocol.
//!
//! 1. **Init**: Initializing of both controller and client state.
//! 2. **Create session**: The controller requests a new training session on both aggregators.
//! 3. **Training round**: Repeat the following:
//!     1. The controller creates a training round and transmits the task id to the clients.
//!     2. The clients train on their local dataset and submit the resulting gradients to the aggregators.
//!     3. The controller collects the aggregated gradient vector from the leader aggregator.
//! 4. **End session**: End the training session.
//!
//! ## 1. Init
//! An initial controller and client state has to be generated by calling [controller::api_new_controller_state] and [client::api_new_client_state], respectively.
//!
//! ## 2. Create session
//! A training session stores configuration data persisting between individual training rounds.
//! Before training can begin, a new session has to be created by the controller by calling [controller::api_create_session].
//!
//! ## 3. Training round
//!
//! First, the controller requests both aggregators to start a new round (controller::api_start_round).
//! The task id of this round
//! has to be communicated to the clients. This has to be done indepedently of dpsa4fl, for example
//! by utilizing the federation framework. Furthermore, the current gradient has to be communicated to the
//! clients as well.
//! ```text
//!                         ┌─────────────────────┐
//!                    ┌───►│ Aggregator (leader) │
//!                    │    └─────────────────────┘
//!  api_start_round() │
//!                    │    ┌─────────────────────┐
//!                    ├───►│ Aggregator (helper) │
//!                    │    └─────────────────────┘
//!                    │
//!                    │
//!   ┌──────────┐     │                                     ┌────────────┐
//!   │Controller├─────┘                                     │ Clients... │
//!   └─────┬────┘                                           └────────────┘
//!         │                                                      ▲
//!         │                                                      │
//!         └──────────────────────────────────────────────────────┘
//!             submit current gradient & task id (out of band)
//!  ```
//!
//! Once the clients have the current gradient, they can train on their local dataset.
//! The resulting gradient is submitted to the aggregators by using the [client::api_submit_with]
//! function which requires the task id of this round as argument. Meanwhile, the controller
//! calls [controller::api_collect] to wait for, and receive the aggregated gradients once they are
//! computed by the aggregators.
//! ```text
//!                         ┌─────────────────────┐
//!                    ┌────┤ Aggregator (leader) │◄────┐
//!                    │    └─────────────────────┘     │
//!      api_collect() │                                │ api_submit_with()
//!                    │    ┌─────────────────────┐     │
//!                    │    │ Aggregator (helper) │◄────┤
//!                    │    └─────────────────────┘     │
//!                    │                                │
//!                    │                                │
//!   ┌──────────┐     │                                │    ┌────────────┐
//!   │Controller│◄────┘                                └────┤ Clients... │
//!   └──────────┘                                           └────────────┘
//! ```
//!
//! ## 4. End session
//! After successfull completion of learning, the persistent state has to be deleted by calling [controller::api_end_session].
//!

/// API for clients. This is for getting configuration from the aggregation servers and submitting
/// gradients.
pub mod client;

/// API for the controller. This is for managing dpsa4fl-janus sessions, and collecting aggregated
/// gradients.
pub mod controller;

/// Definitions of core datastructures.
pub mod core;

/// Helper functions useful for binding crates.
pub mod helpers;
